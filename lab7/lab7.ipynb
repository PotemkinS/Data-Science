{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "237fd82e-724e-41d1-81e4-20100e499920",
   "metadata": {},
   "source": [
    "# Лабораторная 7. Сентимент-анализ\n",
    "\n",
    "Задачи классификации текста аналогично обычной задаче классификации предполагает присвоение метки класса некоторому тексту. Здесь можно действовать любыми методами для того, чтобы классифицировать текст, но мы пойдем по следующему пути: векторизуем последовательности (обязательно почитайте про подходы к векторизации, об эмбеддингах) и обучим RNN\n",
    "\n",
    "После обучения базовых моделей разрешается использовать любой другой подход\n",
    "\n",
    "За выполнение базовой работы можно получить 15 баллов, за преодоление отметки в 94% точности классификации еще 5 баллов\n",
    "Удачи!\n",
    "\n",
    "Примечание: обязательно почитайте про лемматизацию, стеминг, TF-IDF и Word2Vec подходы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9377bbbd-cca4-45be-a5d6-2999885bf61e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from tqdm.auto import tqdm \n",
    "\n",
    "\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re \n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import torch \n",
    "import torch.nn as nn  \n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader , TensorDataset\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "lb = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d79b016-b310-41b8-8a48-85023563689b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34b5dd3-28b7-4c6c-9bb7-6c2a4f5c3d1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('twitter_training.csv', header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e720b-7b9d-4638-a23a-bf05fe7cc116",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique(df[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff77261-3daa-46ac-a45c-a7d5c874b23f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop(0 , axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a09cb08-b4ea-48db-ab1e-d4bbd6ac507e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df= df.rename(columns={1:\"Feature2\",3:\"Feature1\",2: \"labels\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df932029-8aa9-4e5a-807b-ed4c731092fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4865e27-ddc1-4185-9b9e-a8608659a8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"tweets\"]= df[\"Feature1\"].astype(str) +\" \"+ df[\"Feature2\"].astype(str)\n",
    "df= df.drop([\"Feature1\",\"Feature2\"],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfd36ef-dd09-4466-8a13-31a89fdc114d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labels = {key : value for value , key in enumerate(np.unique(df['labels']))}\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c234fc-dae7-40e0-a9d0-aadafb8795fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getlabel(n) : \n",
    "    for x , y in df_labels.items() : \n",
    "        if y==n : \n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c83fa9-5a14-466a-aebd-c1a8ef4ed15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DataPrep(text) : \n",
    "    text = re.sub('<.*?>', '', text) # HTML tags\n",
    "    text = re.sub(r'\\d+', '', text) # numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # special characters\n",
    "    text = re.sub(r'http\\S+', '', text) # URLs or web links\n",
    "    text = re.sub(r'@\\S+', '', text) # mentions\n",
    "    text = re.sub(r'#\\S+', '', text) # hashtags\n",
    "    \n",
    "    # tokenization \n",
    "    tokens = nltk.word_tokenize(text) \n",
    "    \n",
    "    # remove puncs \n",
    "    punc = list(punctuation)\n",
    "    words = [word for word in tokens if word not in punc]\n",
    "    \n",
    "    # remove stopwords \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if not word.lower() in stop_words]\n",
    "    \n",
    "    # Lemmatization \n",
    "    words = [lemma.lemmatize(word) for word in words] \n",
    "    \n",
    "    text = ' '.join(words)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97998a97-bb73-40a9-b346-238fa9de85ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['cleaned_tweets'] = df['tweets'].apply(DataPrep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a544e64-ca2b-4733-bdee-52857485ec43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'There are around {int(df[\"cleaned_tweets\"].duplicated().sum())} duplicated tweets, we will remove them.')\n",
    "df.drop_duplicates(\"cleaned_tweets\", inplace=True)\n",
    "df['tweet_len'] = [len(text.split()) for text in df.cleaned_tweets]\n",
    "df = df[df['tweet_len'] < df['tweet_len'].quantile(0.995)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2ba871-5e42-4f6e-bfa2-df3bbae1df9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,5))\n",
    "ax = sns.countplot(x='tweet_len', data=df[(df['tweet_len']<=1000) & (df['tweet_len']>10)], palette='Blues_r')\n",
    "plt.title('Count of tweets with high number of words', fontsize=25)\n",
    "plt.yticks([])\n",
    "ax.bar_label(ax.containers[0])\n",
    "plt.ylabel('count')\n",
    "plt.xlabel('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d38d93-ce3b-41b9-b15c-f5a23cff226e",
   "metadata": {},
   "source": [
    "## Обучаем модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6956de-978a-4baf-846b-ab6369b08565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = np.max(df['tweet_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb165366-48d9-4c16-9523-4a268fffdc74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lstm_prep(column , seq_len) : \n",
    "    # create a vocab of words \n",
    "    corpus = [word for text in column for word in text.split()]\n",
    "    words_count = Counter(corpus) \n",
    "    sorted_words = words_count.most_common()\n",
    "    vocab_to_int = {w:i+1 for i , (w,c) in enumerate(sorted_words)}\n",
    "    \n",
    "    text_int = [] \n",
    "    \n",
    "    for text in column : \n",
    "        token = [vocab_to_int[word] for word in text.split()]\n",
    "        text_int.append(token)\n",
    "        \n",
    "        \n",
    "    # padding \n",
    "    features = np.zeros((len(text_int) , seq_len) , dtype = int)\n",
    "    for idx , y in tqdm(enumerate(text_int)) : \n",
    "        if len(y) <= seq_len : \n",
    "            zeros = list(np.zeros(seq_len - len(y)))\n",
    "            new = zeros + y\n",
    "            \n",
    "        else : \n",
    "            new = y[:seq_len]\n",
    "            \n",
    "        features[idx,:] = np.array(new)\n",
    "        \n",
    "    return sorted_words, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de28571c-3aec-4d38-b01c-ccd6f43d2400",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB , tokenized_column = lstm_prep(df['cleaned_tweets'] , MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc11ded-cc0a-40df-84e1-bec1e54721ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b811eee9-dffa-4a38-96f4-76d97971267d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_column.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b0be6-eac8-4c99-b48b-47b770eb3720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def most_common_words(vocab) : \n",
    "    keys = [] \n",
    "    values = [] \n",
    "    for key , value in vocab[:30] : \n",
    "        keys.append(key) \n",
    "        values.append(value)\n",
    "        \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax = plt.bar(keys, values)\n",
    "    plt.title('Top 20 most common words', size=25)\n",
    "    plt.ylabel(\"Words count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.subplots_adjust(bottom=0.15)\n",
    "    plt.show()\n",
    "    \n",
    "most_common_words(VOCAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44bb059-aaab-4767-9307-7fcb8fd2380d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = tokenized_column\n",
    "y = lb.fit_transform(df['labels'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4099d1d5-c88d-4c49-b0f0-32d421a5c2ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train , X_val , Y_train , Y_val = train_test_split(X , y , train_size=0.85 , random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fc9f98-8aba-4af7-80c6-f16a71227b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = TensorDataset(torch.from_numpy(X_train), torch.LongTensor(Y_train))\n",
    "val_data = TensorDataset(torch.from_numpy(X_val), torch.LongTensor(Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36540bdd-1e95-46ee-895d-51d3826f4f01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(\n",
    "    dataset = train_data , \n",
    "    batch_size=BATCH_SIZE , \n",
    "    shuffle=True\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset = val_data , \n",
    "    batch_size = BATCH_SIZE , \n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5a08c-d8de-4172-ad56-4c573ada9189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164d0ba-a694-4cf9-aca0-c0ed5fee392d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Word2vec_train_data = list(map(lambda x: x.split(), df['cleaned_tweets']))\n",
    "word2vec_model = Word2Vec(Word2vec_train_data, vector_size=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad971995-31e4-45e6-8cda-fbe9c4952d19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weight_matrix(model,vocab):\n",
    "    vocab_size= len(vocab)+1\n",
    "    embedding_matrix = np.zeros((vocab_size,EMBEDDING_DIM))\n",
    "    for word, token in vocab:\n",
    "        if model.wv.__contains__(word):\n",
    "            embedding_matrix[token]=model.wv.__getitem__(word)\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85a4bf-c99a-4d24-ba23-81b5371608c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_vec = weight_matrix(word2vec_model,VOCAB)\n",
    "print(\"Embedding Matrix Shape:\", embedding_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e15256-74c3-4963-8bb3-da6c9cd7e8b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def param_count(model):\n",
    "    params = [p.numel() for p in model.parameters() if p.requires_grad]\n",
    "    print('The Total number of parameters in the model : ', sum(params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8664ab03-0749-4896-8f15-8d152302cc70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module) : \n",
    "    def __init__(self , vocab_size , embedding_dim \n",
    "                 , num_layers , hidden_dim , out_channels , bidirectional, device='cpu') : \n",
    "        super().__init__() \n",
    "        print(device)\n",
    "        self.no_layers = num_layers \n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.out_channels = out_channels\n",
    "        self.num_directions = 2 if bidirectional else 1  \n",
    "        self.embedding = nn.Embedding(vocab_size , embedding_dim)\n",
    "        self.embedding = self.embedding.to(device)\n",
    "        self.device = device\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim , \n",
    "            hidden_dim , \n",
    "            num_layers , \n",
    "            dropout = 0.5 , \n",
    "            bidirectional = bidirectional , \n",
    "            batch_first = True\n",
    "        )\n",
    "        self.lstm = self.lstm.to(device)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim*self.num_directions , out_channels)\n",
    "        self.fc = self.fc.to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self , x) : \n",
    "        h0 = torch.zeros((self.no_layers * self.num_directions , x.size(0) , self.hidden_dim)).to(self.device)\n",
    "        c0 = torch.zeros((self.no_layers * self.num_directions , x.size(0) , self.hidden_dim)).to(self.device)\n",
    "        \n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        out , _ = self.lstm(embedded , (h0 , c0))\n",
    "        \n",
    "        out = out[:,-1,:]\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be721827-b1f7-4987-8b09-9f29cdcb280f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(VOCAB) + 1\n",
    "NUM_LAYERS = 2 \n",
    "OUT_CHANNELS = 4 \n",
    "HIDDEN_DIM = 256\n",
    "BIDIRECTIONAL = True\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = Model(VOCAB_SIZE , EMBEDDING_DIM , NUM_LAYERS , HIDDEN_DIM , OUT_CHANNELS , BIDIRECTIONAL, DEVICE)\n",
    "\n",
    "model.embedding.weight.data.copy_(torch.from_numpy(embedding_vec))\n",
    "\n",
    "model.embedding.weight.requires_grad = True\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afed9a4f-e5b6-4afa-b994-6c77a086b621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_count(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65df6e07-513d-4c71-8ece-2f2eb4c3a94c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=Adam(model.parameters(),lr=0.001)\n",
    "\n",
    "epochs = 10 \n",
    "training_loss = []\n",
    "training_acc = [] \n",
    "for i in tqdm(range(epochs)) : \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    for batch , (x_train , y_train) in enumerate(train_dataloader) :\n",
    "        x_train , y_train = x_train.to(DEVICE), y_train.to(DEVICE)\n",
    "        y_pred = model(x_train)\n",
    "        \n",
    "        loss = criterion(y_pred , y_train) \n",
    "        \n",
    "        if batch % 500 == 0:\n",
    "            print(f\"Looked at {batch * len(x_train)}/{len(train_dataloader.dataset)} samples.\")\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        epoch_loss +=loss \n",
    "        epoch_acc += accuracy_score(y_train.cpu() , y_pred.argmax(dim=1).cpu())\n",
    "        \n",
    "    training_loss.append((epoch_loss/len(train_dataloader)).detach().cpu().numpy())\n",
    "    training_acc.append(epoch_acc/len(train_dataloader))\n",
    "    \n",
    "    print(f\"Epoch {i+1}: Accuracy: {(epoch_acc/len(train_dataloader)) * 100}, Loss: {(epoch_loss/len(train_dataloader))}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a6048c-b946-4351-9c6a-04bbd9e980d8",
   "metadata": {},
   "source": [
    "## Время Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad25633-d40c-4499-9081-cae5963f24a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('twitter_validation.csv', header=None)\n",
    "print(test_df.head())\n",
    "test_df = test_df.drop(0 , axis=1)\n",
    "\n",
    "test_df = test_df.rename(columns={1:\"Feature2\",3:\"Feature1\",2: \"labels\"})\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5e398-a443-46a4-bcdb-c848d628de78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df[\"tweets\"]= test_df[\"Feature1\"].astype(str) +\" \"+ test_df[\"Feature2\"].astype(str)\n",
    "test_df= test_df.drop([\"Feature1\",\"Feature2\"],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b16d583-63b3-4e18-8f32-beb87b25172f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b7c6e-21fa-4b56-ac0c-10a016832770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_predictions(row) : \n",
    "    random_data = row.sample(n=10)\n",
    "    random_tweets = random_data['tweets'].values\n",
    "    \n",
    "    cleaned_tweets = [] \n",
    "    for tweet in random_tweets : \n",
    "        cleaned_tweets.append(DataPrep(tweet))\n",
    "        \n",
    "    x_test = vec.transform(cleaned_tweets).toarray()\n",
    "    \n",
    "    y_test = random_data['labels'].values\n",
    "        \n",
    "    _ , X_test = lstm_prep(cleaned_tweets , MAX_LEN)\n",
    "    \n",
    "    X_test = torch.from_numpy(X_test).to(DEVICE)\n",
    "\n",
    "    lstm_pred = model(X_test)\n",
    "    lstm_pred = torch.softmax(lstm_pred , dim=1 ).argmax(dim=1)\n",
    "    pred = np.array([getlabel(lstm_pred[i]) for i in range(len(lstm_pred))])\n",
    "    for i in tqdm(range(2)) : \n",
    "        print(f\"The original tweet : {random_tweets[i]}\\n\")\n",
    "        print(f\"The original label : {y_test[i]}\\n\")\n",
    "        print(f\"The lstm prediction is : {getlabel(lstm_pred[i])}\\n\")\n",
    "        print('-'*120)\n",
    "    print(accuracy_score(pred, y_test))\n",
    "    \n",
    "    \n",
    "make_predictions(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad62011a-cd59-442a-8133-4e61b75ecccc",
   "metadata": {},
   "source": [
    "## Что дальше?\n",
    "\n",
    "Попытайтесь улучшить модель (попробуйте GRU), изменить подход к токенизации данных и так далее, удачи!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
